{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDsVMGiVgSq2"
   },
   "source": [
    "## Локальные дескрипторы изображений и построение панорам\n",
    "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), https://t.me/s/girafe_ai\n",
    "\n",
    "**Примечание: т.к. библиотеки регулярно обновляются, для успешного прохождения тестов рекомендуем поставить opencv версии 4.11, иначе некоторые asssert'ы могут не пройти.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3isBRG6PgSq6"
   },
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from skimage import io # for io.imread\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors # ploting\n",
    "\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "def imshow(images, titles, nrows = 0, ncols=0, figsize = (15,20)):\n",
    "    \"\"\"Plot a multiple images with titles.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    images : image list\n",
    "    titles : title list\n",
    "    ncols : number of columns of subplots wanted in the display\n",
    "    nrows : number of rows of subplots wanted in the figure\n",
    "    \"\"\"\n",
    "\n",
    "    if ncols == 0 and nrows == 0:\n",
    "      ncols = len(images)\n",
    "      nrows = 1\n",
    "    if ncols == 0:\n",
    "      ncols = len(images) // nrows\n",
    "    if nrows == 0:\n",
    "      nrows = len(images) // ncols\n",
    "\n",
    "    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows, squeeze=False, figsize = figsize)\n",
    "    for i, image in enumerate(images):\n",
    "        axeslist.ravel()[i].imshow(image, cmap=plt.gray(), vmin=0, vmax=255)\n",
    "        axeslist.ravel()[i].set_title(titles[i])\n",
    "        axeslist.ravel()[i].set_axis_off()\n",
    "    plt.tight_layout() # optional\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./images'):\n",
    "    os.mkdir('./images')\n",
    "else:\n",
    "    print('folder `images` is present, passing')\n",
    "\n",
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example1_1.jpeg\n",
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example1_2.jpeg\n",
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example2_1.jpeg\n",
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example2_2.jpeg\n",
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example3_1.jpeg\n",
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example3_2.jpeg\n",
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example4_1.jpeg\n",
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example4_2.jpeg\n",
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example5_1.jpeg\n",
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example5_2.jpeg\n",
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example6_1.jpeg\n",
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example6_2.jpeg\n",
    "!mv ./example*.jpeg images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/keypoints_sift.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "# Create a dictionary to store images with the same prefix\n",
    "image_dict = {}\n",
    "\n",
    "# Read all images and group them by prefix\n",
    "for filename in sorted(glob.glob('./images/*.jpeg')):\n",
    "    name = os.path.basename(filename)\n",
    "    prefix = name.split('_')[0]  # Get prefix before first underscore\n",
    "    \n",
    "    # Load the image\n",
    "    img = io.imread(filename)\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Add to dictionary\n",
    "    if prefix in image_dict:\n",
    "        image_dict[prefix].append((img))\n",
    "    else:\n",
    "        image_dict[prefix] = [(img)]\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "image1, image2 = image_dict['example2']\n",
    "imshow( [image1, image2], ['Left', 'Right'])\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 1: Создание панорамы вручную\n",
    "Подберите сдвиг по координатам X и Y (`tx` и `ty` соответственно) для второго изобаражения относительно первого для задания оптимальной трансляции (смещения) для совмещения изображений вручную. Изображения имеют размеры, приведенные ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"first image shape: {image1.shape}, second image shape: {image2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SHIFT = 100\n",
    "Y_SHIFT = 100 # so you can have negative tx and ty \n",
    "tx = 0 # YOUR CODE HERE\n",
    "ty = 0 # YOUR CODE HERE\n",
    "\n",
    "assert tx + X_SHIFT >= 0\n",
    "assert ty + Y_SHIFT >= 0\n",
    "\n",
    "size = (image1.shape[0] + image2.shape[0], image1.shape[1] + image2.shape[1], 3)\n",
    "image_trans = np.uint8(np.zeros(size))\n",
    "\n",
    "# put image 1 on resulting image\n",
    "image_trans[Y_SHIFT:Y_SHIFT+image1.shape[0], X_SHIFT:X_SHIFT+image1.shape[1], :] = image1\n",
    "\n",
    "# put image 2 on resulting image\n",
    "image_trans[Y_SHIFT+ty:Y_SHIFT+ty+image2.shape[0], X_SHIFT+tx:X_SHIFT+tx+image2.shape[1], :] = image2\n",
    "\n",
    "# #add vertical line where two images are joined, use red color\n",
    "image_trans[:, X_SHIFT+tx, :] = [255, 0, 0]\n",
    "image_trans[Y_SHIFT+ty, :, :] = [0, 255, 0]\n",
    "\n",
    "imshow( [image_trans], ['Translation-based panorama'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "with open('manual_panorama.json', 'w') as iofile:\n",
    "    json.dump([tx, ty], iofile)\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы можете сдать файл `manual_panorama.json` в задачу **Manual panorama** в соревновании."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 2: Обнаружение ключевых точек\n",
    "\n",
    "Используйте  **SIFT** из OpenCV (`cv2.SIFT_create`) для обнаружения ключевых точек и вычисления их дескрипторов на обоих изображениях.  \n",
    "Для этого реализуйте функцию `extract_key_points`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_points(img1, img2):\n",
    "    # YOUR CODE HERE\n",
    "    kpts1, desc1 = None, None\n",
    "    kpts2, desc2 = None, None\n",
    "    return kpts1, desc1, kpts2, desc2\n",
    "\n",
    "kp1, des1, kp2, des2 = extract_key_points(image1, image2)\n",
    "\n",
    "\n",
    "print(\"Coordinates of the first keypoint of image1: \", kp1[0].pt)\n",
    "print(\"Descriptor of the first keypoint of image1:\\n\", des1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "with open('keypoints_sift.json', 'r') as f:\n",
    "    loaded_data = json.load(f)\n",
    "\n",
    "for kp, loaded_kp in zip(kp1[:10], loaded_data['keypoints1']):\n",
    "    assert np.allclose(kp.pt, loaded_kp, atol=1e-5), f\"keypoint {kp.pt} and {loaded_kp} are not close\"\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 3: Сопоставление ключевых точек  \n",
    "Далее необходимо сопоставить признаки между изображениями. Существует множество стратегий сопоставления, давайте используем самый простой подход, реализованный в [BFMatcher](https://opencv24-python-tutorials.readthedocs.io/en/stable/py_tutorials/py_feature2d/py_matcher/py_matcher.html) (от Brute Force).\n",
    "\n",
    "Оценить качество сопоставления ключевых точек можно на основе аттрибута `distance`. Полезно отсортировать соответствия по возрастанию расстояния, чтобы первые элементы списка были наиболее релевантными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "def match_key_points_cv(des1, des2):\n",
    "    bf =  cv2.BFMatcher(crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "\n",
    "    sorted_matches = sorted(matches, key = lambda x:x.distance)\n",
    "    return sorted_matches\n",
    "\n",
    "def showMatches(img1, kp1, img2, kp2, matches, name):\n",
    "    img = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    imshow([img],[name])\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_cv = match_key_points_cv(des1, des2)\n",
    "\n",
    "print(len(matches_cv))\n",
    "showMatches(image1,kp1,image2,kp2,matches_cv,\"all matches\")\n",
    "showMatches(image1,kp1,image2,kp2,matches_cv[:50],\"best 50 matches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваша задача: реализовать аналог `BFMatcher`, используя чистый `numpy`. Он основан на подсчете разницы между дескрипторами изображений и вычисляется следующим образом:\n",
    "\n",
    "1. Подсчет матрицы попарных расстояний между дескрипторами обоих изображений.\n",
    "2. Для каждого дескриптора изображения 1 находится наиболее похожий (ближайший) дескриптор изображения 2.\n",
    "2. Для каждого дескриптора изображения 2 находится наиболее похожий (ближайший) дескриптор изображения 1.\n",
    "4. Те пары дескрипторов, которые совпали (т.е. дескрипторы ближайшие друг для друга) считаются парами.\n",
    "\n",
    "Шаблоны кода написаны ниже. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "class DummyMatch:\n",
    "    def __init__(self, queryIdx, trainIdx, distance):\n",
    "        self.queryIdx = queryIdx  # index in des1\n",
    "        self.trainIdx = trainIdx  # index in des2\n",
    "        self.distance = distance\n",
    "# __________end of block__________\n",
    "\n",
    "\n",
    "def match_key_points_numpy(des1: np.ndarray, des2: np.ndarray) -> list:\n",
    "    \"\"\"\n",
    "    Match descriptors using brute-force matching with cross-check.\n",
    "\n",
    "    Args:\n",
    "        des1 (np.ndarray): Descriptors from image 1, shape (N1, D)\n",
    "        des2 (np.ndarray): Descriptors from image 2, shape (N2, D)\n",
    "\n",
    "    Returns:\n",
    "        List[DummyMatch]: Sorted list of mutual best matches.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    matches = []\n",
    "\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "def test_numpy_bf_matcher_equivalence(des1, des2):\n",
    "    # OpenCV BFMatcher\n",
    "    cv_matches = match_key_points_cv(des1, des2)\n",
    "    \n",
    "    # Our matcher\n",
    "    np_matches = match_key_points_numpy(des1, des2)\n",
    "\n",
    "    # Compare match indices and distances\n",
    "    assert len(cv_matches) == len(np_matches), f\"Match count mismatch: {len(cv_matches)} vs {len(np_matches)}\"\n",
    "\n",
    "    for idx, (m_cv, m_np) in enumerate(zip(cv_matches, np_matches)):\n",
    "        assert m_cv.queryIdx == m_np.queryIdx\n",
    "        assert m_cv.trainIdx == m_np.trainIdx\n",
    "        assert abs(m_cv.distance - m_np.distance) < 1e-4, f\"Distance mismatch on {idx}th match: {m_cv.distance:.4f} vs {m_np.distance:.4f}\"\n",
    "\n",
    "    print(\"Your numpy implementation matches OpenCV BFMatcher output!\")\n",
    "\n",
    "test_numpy_bf_matcher_equivalence(des1, des2)\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сдайте функцию `match_key_points_numpy` в задачу **BFMatcher** в соревновании."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 4 (опциональный): Оценка матрицы гомографии с использованием DLT и RANSAC\n",
    "\n",
    "Функция cv2.findHomography оценивает гомографию, которая преобразует исходные точки в целевые. При использовании метода RANSAC она устойчива к выбросам. Ниже доступна реализация функции с использованием RANSAC, а также с использованием более простого метода DLT.\n",
    "\n",
    "Ваша опциональная задача: реализовать DLT на чистом `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "def findHomography_dlt_opencv(matches, keypoint1, keypoint2, mode='DLT'):\n",
    "\n",
    "    src_pts = np.float32([keypoint1[m.queryIdx].pt for m in matches]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([keypoint2[m.trainIdx].pt for m in matches]).reshape(-1,1,2)\n",
    "\n",
    "    if mode == 'DLT':\n",
    "        mode = 0\n",
    "    elif mode == 'RANSAC':\n",
    "        mode = cv2.RANSAC\n",
    "    H, mask = cv2.findHomography(src_pts, dst_pts, mode)\n",
    "    mask = mask.ravel().tolist()\n",
    "\n",
    "    inliers = []\n",
    "    for i in range(len(mask)):\n",
    "      if mask[i] == 1:\n",
    "        inliers.append(matches[i])\n",
    "\n",
    "    return H, inliers\n",
    "\n",
    "\n",
    "H_for_panorama, inliers = findHomography_dlt_opencv(matches_cv, kp1, kp2, 'RANSAC')\n",
    "showMatches(image1,kp1,image2,kp2,inliers,\"inliers only, RANSAC\")\n",
    "\n",
    "\n",
    "H, inliers = findHomography_dlt_opencv(matches_cv, kp1, kp2, 'DLT')\n",
    "showMatches(image1,kp1,image2,kp2,inliers,\"DLT, all matches\")\n",
    "\n",
    "H, inliers = findHomography_dlt_opencv(matches_cv[:50], kp1, kp2, 'DLT')\n",
    "showMatches(image1,kp1,image2,kp2,inliers,\"DLT, top 50 matches\")\n",
    "\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ручная реализация DLT нетривиальна и опциональна. Пример с развернутым описанием и разбором можно найти, например, [здесь](https://medium.com/@insight-in-plain-sight/estimating-the-homography-matrix-with-the-direct-linear-transform-dlt-ec6bbb82ee2b).\n",
    "Т.к. тонкости реализации могут варьироваться (например, нормировка), оценка похожести матрицы `H`, полученной с помощью вашей реализации и реализации `opencv` достаточно грубая.\n",
    "Рекомендуем опираться также на визуальную проверку сопоставления изображений ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlt_homography_normalized(pts1: np.ndarray, pts2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes homography matrix using normalized Direct Linear Transform (DLT).\n",
    "\n",
    "    Args:\n",
    "        pts1 (np.ndarray): Source points (N, 2)\n",
    "        pts2 (np.ndarray): Destination points (N, 2)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Homography matrix (3x3)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    H = None\n",
    "\n",
    "    return H\n",
    "\n",
    "\n",
    "def findHomography_dlt_numpy(matches, keypoint1, keypoint2):\n",
    "    src_pts = np.float32([keypoint1[m.queryIdx].pt for m in matches]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([keypoint2[m.trainIdx].pt for m in matches]).reshape(-1,1,2)\n",
    "    return dlt_homography_normalized(src_pts, dst_pts), None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. DLT неустойчив к выбросам (и вообще достаточно прост), будем использовать `NUM_BEST_MATCHES` лучших совпадений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "NUM_BEST_MATCHES = 50\n",
    "\n",
    "kp1, des1, kp2, des2 = extract_key_points(*image_dict['example2'])\n",
    "matches_cv = match_key_points_cv(des1, des2)\n",
    "\n",
    "H_numpy, _ = findHomography_dlt_numpy(matches_cv[:NUM_BEST_MATCHES], kp1, kp2)\n",
    "H, _ = findHomography_dlt_opencv(matches_cv[:NUM_BEST_MATCHES], kp1, kp2, mode='DLT')\n",
    "assert np.allclose(H_numpy, H, atol=2e-1), f\"Homography matrices are too different!\\nH_numpy:\\n{H_numpy}\\nH from opencv:\\n{H}\"\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 5. Построение панорамы\n",
    "Используя матрицу гомографии `H` можно совместить две фотографии. Функция реализована для вас. Сравните с результатом, полученным вручную на шаге 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def panorama(img1, img2, H, size):\n",
    "    img = np.uint8(np.zeros(size))\n",
    "    img = cv2.warpPerspective(src=img1, dst=img, M=np.eye(3), dsize=(size[1], size[0]))\n",
    "    img = cv2.warpPerspective(src=img2, dst=img, M=H, dsize=(size[1], size[0]), flags=cv2.WARP_INVERSE_MAP, borderMode=cv2.BORDER_TRANSPARENT)\n",
    "\n",
    "    return img\n",
    "\n",
    "size = (1280, 960*2, 3)\n",
    "imshow([panorama(image1, image2, H_for_panorama, size)],[\"Panorama\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 6: Функция для построения панорам\n",
    "Осталось лишь собрать воедино ваши наработки (кроме ручного DLT) и получить функцию, которая на входе имеет два изображения, а на выходе возвращает совмещенное изображение и матрицу `H`.\n",
    "Благодаря использованию SIFT угол съемки может варьироваться.\n",
    "\n",
    "Обращаем ваше внимание, даже при правильной реализации на одном из примеров панорама не будет строиться корректно (`example_5`), это нормально.\n",
    "\n",
    "**Примечание: т.к. вариантов построения данной функции много, для успешного прохождения тестов в контесте используйте следующую последовательность:**\n",
    "1. Обнаружение ключевых точек с помощью фунции `extract_key_points` с SIFT под капотом (она уже реализована выше).\n",
    "2. Для сопоставления ключевых точек используйте функцию `match_key_points_cv` (она также реализована).\n",
    "3. Для определения матрицы гомографии используйте `findHomography_dlt_opencv` с методом `RANSAC` (также реализована)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def panorama_pipeline(img1, img2, size):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    res = None\n",
    "    H = None\n",
    "\n",
    "    return res, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "size = (1280, 960*2, 3)\n",
    "h_dict = {}\n",
    "\n",
    "for filename, (img1, img2) in image_dict.items():\n",
    "    final_image, H = panorama_pipeline(img1, img2, size)\n",
    "    h_dict[filename] = H.tolist()\n",
    "    imshow([final_image],[filename])\n",
    "\n",
    "with open('h_submission_dict.json', 'w') as iofile:\n",
    "    json.dump(h_dict, iofile)\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сдайте `h_submission_dict.json` в задачу **Homography mapping**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 7: Ваш telegram login\n",
    "Запишите ваш telegram login в формате ссылки https://t.me/username в строку ниже. Он будет нужен для участия в тестах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_telegram_login = None\n",
    "\n",
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert your_telegram_login[:13] == 'https://t.me/'\n",
    "assert '@' not in your_telegram_login\n",
    "\n",
    "with open('telegram_login.json', 'w') as iofile:\n",
    "    json.dump([your_telegram_login], iofile)\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сдача задания\n",
    "Для сдачи задания вам необходимо:\n",
    " * Сдать файл `manual_panorama.json` в задачу **Manual panorama** в соревновании.\n",
    " * Сдать функцию `match_key_points_numpy` в задачу **BFMatching** в соревновании.\n",
    " * Сдать `h_submission_dict.json` в задачу **Homography mapping**.\n",
    " * Сдать `telegram_login.json` в задачу **telegram_login** для участия в тестах.\n",
    "\n",
    "Не забудьте, что при сдаче кода все импортируемые библиотеки также должны вставляться вместе с вашим кодом. Вы не должны использовать что-либо, кроме `numpy` при написании решения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtWnYAN_gSrA"
   },
   "source": [
    "На этом задание завершено. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py3_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
